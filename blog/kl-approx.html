<!DOCTYPE html>
<html>

<!-- Mirrored from joschu.net/blog/kl-approx.html by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 20 Mar 2021 11:24:52 GMT -->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Approximating KL Divergence</title>
<style>
html {
  font-size: 100%;
  overflow-y: scroll;
  -webkit-text-size-adjust: 100%;
  -ms-text-size-adjust: 100%;
}

body {
  color: #444;
  font-family: Georgia, Palatino, 'Palatino Linotype', Times, 'Times New Roman', serif;
  font-size: 12px;
  line-height: 1.4;
  padding: 1em;
  margin: auto;
  max-width: 60em;
  background: #fefefe;
}

a {
  color: #0645ad;
  text-decoration: none;
}

a:visited {
  color: #0b0080;
}

a:hover {
  color: #06e;
}

a:active {
  color: #faa700;
}

a:focus {
  outline: thin dotted;
}

*::-moz-selection {
  background: rgba(255, 255, 0, 0.3);
  color: #000;
}

*::selection {
  background: rgba(255, 255, 0, 0.3);
  color: #000;
}

a::-moz-selection {
  background: rgba(255, 255, 0, 0.3);
  color: #0645ad;
}

a::selection {
  background: rgba(255, 255, 0, 0.3);
  color: #0645ad;
}

p {
  margin: 1em 0;
}

img {
  max-width: 100%;
}

h1, h2, h3, h4, h5, h6 {
  color: #111;
  /* line-height: 125%; */
  /* margin-top: 2em; */
  font-weight: normal;
}

h4, h5, h6 {
  font-weight: bold;
}

h1 {
  font-size: 2.5em;
}

h2 {
  font-size: 2em;
}

h3 {
  font-size: 1.5em;
}

h4 {
  font-size: 1.2em;
}

h5 {
  font-size: 1em;
}

h6 {
  font-size: 0.9em;
}

blockquote {
  color: #666666;
  margin: 0;
  padding-left: 3em;
  border-left: 0.5em #EEE solid;
}

hr {
  display: block;
  height: 2px;
  border: 0;
  border-top: 1px solid #aaa;
  border-bottom: 1px solid #eee;
  margin: 1em 0;
  padding: 0;
}

pre, code, kbd, samp {
  color: #000;
  font-family: monospace, monospace;
  _font-family: 'courier new', monospace;
  font-size: 0.98em;
}

pre {
  white-space: pre;
  white-space: pre-wrap;
  word-wrap: break-word;
}

b, strong {
  font-weight: bold;
}

dfn {
  font-style: italic;
}

ins {
  background: #ff9;
  color: #000;
  text-decoration: none;
}

mark {
  background: #ff0;
  color: #000;
  font-style: italic;
  font-weight: bold;
}

sub, sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}

sup {
  top: -0.5em;
}

sub {
  bottom: -0.25em;
}

ul, ol {
  margin: 1em 0;
  padding: 0 0 0 2em;
}

li p:last-child {
  margin-bottom: 0;
}

ul ul, ol ol {
  margin: .3em 0;
}

dl {
  margin-bottom: 1em;
}

dt {
  font-weight: bold;
  margin-bottom: .8em;
}

dd {
  margin: 0 0 .8em 2em;
}

dd:last-child {
  margin-bottom: 0;
}

img {
  border: 0;
  -ms-interpolation-mode: bicubic;
  vertical-align: middle;
}

figure {
  display: block;
  text-align: center;
  margin: 1em 0;
}

figure img {
  border: none;
  margin: 0 auto;
}

figcaption {
  font-size: 0.8em;
  font-style: italic;
  margin: 0 0 .8em;
}

table {
  margin-bottom: 2em;
  border-bottom: 1px solid #ddd;
  border-right: 1px solid #ddd;
  border-spacing: 0;
  border-collapse: collapse;
}

table th {
  padding: .2em 1em;
  background-color: #eee;
  border-top: 1px solid #ddd;
  border-left: 1px solid #ddd;
}

table td {
  padding: .2em 1em;
  border-top: 1px solid #ddd;
  border-left: 1px solid #ddd;
  vertical-align: top;
}

.author {
  font-size: 1.2em;
  text-align: center;
}

@media only screen and (min-width: 480px) {
  body {
    font-size: 14px;
  }
}
@media only screen and (min-width: 768px) {
  body {
    font-size: 16px;
  }
}
@media print {
  * {
    background: transparent !important;
    color: black !important;
    filter: none !important;
    -ms-filter: none !important;
  }

  body {
    font-size: 12pt;
    max-width: 100%;
  }

  a, a:visited {
    text-decoration: underline;
  }

  hr {
    height: 1px;
    border: 0;
    border-bottom: 1px solid black;
  }

  a[href]:after {
    content: " (" attr(href) ")";
  }

  abbr[title]:after {
    content: " (" attr(title) ")";
  }

  .ir a:after, a[href^="javascript:"]:after, a[href^="#"]:after {
    content: "";
  }

  pre, blockquote {
    border: 1px solid #999;
    padding-right: 1em;
    page-break-inside: avoid;
  }

  tr, img {
    page-break-inside: avoid;
  }

  img {
    max-width: 100% !important;
  }

  @page :left {
    margin: 15mm 20mm 15mm 10mm;
}

  @page :right {
    margin: 15mm 10mm 15mm 20mm;
}

  p, h2, h3 {
    orphans: 3;
    widows: 3;
  }

  h2, h3 {
    page-break-after: avoid;
  }
}
/* Add a black background color to the top navigation */
.topnav {
  background-color: #333;
  overflow: hidden;
}

/* Style the links inside the navigation bar */
.topnav a {
  float: left;
  color: #f2f2f2;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
  font-size: 17px;
}

.date {
  color: gray
}
</style>

<link rel="stylesheet" href="../static/katex/katex.min.css" />


<script>
document.addEventListener("DOMContentLoaded", function () {
    katexRender();
});
var katexRender = function() {
    var mathElements = document.getElementsByClassName("math");
    var macros = {};
    for (var i = 0; i < mathElements.length; i++) {
        var texText = mathElements[i].firstChild;
        if (mathElements[i].tagName == "SPAN") { 
        katex.render(texText.data, mathElements[i], { 
            displayMode: mathElements[i].classList.contains("display"), 
            throwOnError: false,
            macros: macros } );
        }
    }
};
</script>
<script src="../static/katex/katex.min.js" /></script>

</head>
<body>
<div class="topnav">
  <a href="../index-2.html">John Schulman's Homepage</a>
</div>

    <h2>Approximating KL Divergence</h2>


    <div class="date">Posted on 2020/03/07</div>
    <a href="../blog.html">← back to blog index</a>

<p><span class="math display">
\gdef\ratio{\tfrac{p(x)}{q(x)}}
\gdef\iratio{\tfrac{q(x)}{p(x)}}
\gdef\half{\tfrac{1}{2}}
\gdef{\klqp}{\mathrm{KL}[q,p]}
\gdef{\klpq}{\mathrm{KL}[p,q]}
</span></p>
<p>This post is about Monte-Carlo approximations of KL divergence. <span class="math display">
KL[q, p] = \sum_x q(x) \log \iratio = E_{ x \sim q}[\log \iratio ]
</span> It explains a trick I’ve used in various code, where I approximate <span class="math inline">\klqp</span> as a sample average of <span class="math inline">\half (\log p(x) - \log q(x))^2</span>, for samples <span class="math inline">x</span> from <span class="math inline">q</span>, rather the more standard <span class="math inline">\log \frac{q(x)}{p(x)}</span>. This post will explain why this expression is a good (though biased) estimator of KL, and how to make it unbiased while preserving its low variance.</p>
<p>Our options for computing KL depend on what kind of access we have to <span class="math inline">p</span> and <span class="math inline">q</span>. Here, we’ll be assuming that we can compute the probabilities (or probability densities) <span class="math inline">p(x)</span> and <span class="math inline">q(x)</span> for any <span class="math inline">x</span>, but we can’t calculate the sum over <span class="math inline">x</span> analytically. Why wouldn’t we be able to calculate it analytically?</p>
<ol type="1">
<li>Computing it exactly requires too much computation or memory.</li>
<li>There’s no closed form expression.</li>
<li>We can simplify code by just storing the log-prob, not the whole distribution. This is a reasonable choice if KL is just being used as a diagnostic, as is often the case in reinforcement learning.</li>
</ol>
<p>The most common strategy for estimating sums or integrals is to use a Monte-Carlo estimate. Given samples <span class="math inline">x_1, x_2, \dots \sim q</span>, how can we construct a good estimate?</p>
<p>A good estimator is unbiased (it has the right mean) and has low variance. We know that one unbiased estimator (under samples from <span class="math inline">q</span>) is <span class="math inline">\log \iratio</span>. However, it has high-variance, as it’s negative for half of the samples, whereas KL is always positive. Let’s call this naive estimator <span class="math inline">k_1 = \log \iratio = - \log r</span>, where we’ve defined the ratio <span class="math inline">r=\ratio</span> that’ll appear frequently in the subsequent calculations.</p>
<p>An alternative estimator, which has lower variance but is biased, is <span class="math inline">\frac{1}{2}(\log \ratio)^2 = \half (\log r)^2</span>. Let’s call this estimator <span class="math inline">k_2</span>. Intuitively, <span class="math inline">k_2</span> seems to be better because each sample tells you how far apart <span class="math inline">p</span> and <span class="math inline">q</span> are, and it’s always positive. Empirically, <span class="math inline">k_2</span> does indeed have much lower variance than <span class="math inline">k_1</span>, and also has remarkably low bias. (We’ll show this in an experiment below.)</p>
<p>There’s a good reason why estimator <span class="math inline">k_2</span> has low bias: its expectation is an <a href="https://en.wikipedia.org/wiki/F-divergence">f-divergence</a>. An f-divergence is defined as <span class="math inline">D_f(p,q) = E_{x \sim q}[f(\ratio)]</span> for a convex function <span class="math inline">f</span>. KL divergence and various other well-known probability distances are f-divergences. Now here’s the key non-obvious fact: all f-divergences with differentiable <span class="math inline">f</span> look like KL divergence up to second order when <span class="math inline">q</span> is close to <span class="math inline">p</span>. Namely, for a parametrized distribution <span class="math inline">p_{\theta}</span>,</p>
<p><span class="math display">
D_f(p_0, p_{\theta}) = \tfrac{f&#39;&#39;(1)}{2} \theta^T F \theta + O(\theta^3)
</span></p>
<p>where <span class="math inline">F</span> is the Fisher information matrix for <span class="math inline">p_{\theta}</span> evaluated at <span class="math inline">p_{\theta}=p_0</span>.</p>
<p><span class="math inline">E_q[k_2]=E_q[\frac{1}{2}(\log r)^2]</span> is the f-divergence where <span class="math inline">f(x)=\half (\log x)^2</span>, whereas <span class="math inline">\klqp</span> corresponds to <span class="math inline">f(x)= - \log x</span>. It’s easy to check that both have <span class="math inline">f&#39;&#39;(1)=1</span>, so both look like the same quadratic distance function for <span class="math inline">p\approx q</span>.</p>
<p>Is it possible to write down a KL divergence estimator that is unbiased but also low variance? The general way to lower variance is with a control variate. I.e., take <span class="math inline">k_1</span> and add something that has expectation zero but is negatively correlated with <span class="math inline">k_1</span>. The only interesting quantity that’s guaranteed to have zero expectation is <span class="math inline">\ratio - 1 = r-1</span>. So for any <span class="math inline">\lambda</span>, the expression <span class="math inline">-\log r + \lambda (r - 1)</span> is an unbiased estimator of <span class="math inline">\klqp</span>. We can do a calculation to minimize the variance of this estimator and solve for <span class="math inline">\lambda</span>. But unfortunately we get an expression that depends on <span class="math inline">p</span> and <span class="math inline">q</span> and is hard to calculate analytically.</p>
<p>However, we can choose a good <span class="math inline">\lambda</span> using a simpler strategy. Note that since log is concave, <span class="math inline">\log(x) \le x - 1</span>. Therefore, if we let <span class="math inline">\lambda=1</span>, the expression above is guaranteed to be positive. It measures the vertical distance between <span class="math inline">\log(x)</span> and its tangent. This leaves us with the estimator <span class="math inline">k_3 = (r - 1) - \log r</span>.</p>
<p>The idea of measuring distance by looking at the difference between a convex function and its tangent plane appears in many places. It’s called a <a href="https://en.wikipedia.org/wiki/Bregman_divergence">Bregman divergence</a> and has many beautiful properties.</p>
<p>We can generalize the above idea to get a good, always-positive estimator for any f-divergence, most notably the other KL divergence <span class="math inline">\klpq</span> (note that <span class="math inline">p</span> and <span class="math inline">q</span> are switched here). Since <span class="math inline">f</span> is by convex, and and <span class="math inline">E_q[r]=1</span>, the following is an estimator of the f-divergence: <span class="math inline">f(r) - f&#39;(1)(r-1)</span>. This is always positive because it’s the distance between <span class="math inline">f</span> and its tangent at <span class="math inline">r=1</span>, and convex functions lie above their tangent lines. Now <span class="math inline">\klpq</span> corresponds to <span class="math inline">f(x)=x \log x</span>, which has <span class="math inline">f&#39;(1)=1</span>, leaving us with the estimator <span class="math inline">r \log r - (r - 1)</span>.</p>
<p>In summary, we have the following estimators (for samples <span class="math inline">x \sim q</span>, and <span class="math inline">r = \ratio</span>):</p>
<ul>
<li><span class="math inline">\klpq: r \log r - (r - 1)</span></li>
<li><span class="math inline">\klqp: (r - 1) - \log r</span></li>
</ul>
<p>Now let’s compare the bias and variance of the three estimators for <span class="math inline">\klqp</span>. Suppose <span class="math inline">q=N(0,1)</span>, <span class="math inline">p=N(0.1,1)</span>. Here, the true KL is 0.005.</p>
<table>
<th>
<td>
bias/true
</td>
<td>
stdev/true
</td>
</th>
<tr>
<td>
k1
</td>
<td>
0
</td>
<td>
20
</td>
</tr>
<tr>
<td>
k2
</td>
<td>
0.002
</td>
<td>
1.42
</td>
</tr>
<tr>
<td>
k3
</td>
<td>
0
</td>
<td>
1.42
</td>
</tr>
</table>
<p>Note that the bias of k2 is incredibly low here: it’s 0.2%.</p>
<p>Now let’s try for a larger true KL divergence. <span class="math inline">p=N(1,1)</span> gives us a true KL divergence of 0.5.</p>
<table>
<th>
<td>
bias/true
</td>
<td>
stdev/true
</td>
</th>
<tr>
<td>
k1
</td>
<td>
0
</td>
<td>
2
</td>
</tr>
<tr>
<td>
k2
</td>
<td>
0.25
</td>
<td>
1.73
</td>
</tr>
<tr>
<td>
k3
</td>
<td>
0
</td>
<td>
1.7
</td>
</tr>
</table>
<p>Here, the bias of k2 is much larger. k3 has even lower standard deviation than k2 while being unbiased, so it appears to be a strictly better estimator.</p>
<p>Here’s the code I used to get these results:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> torch.distributions <span class="im">as</span> dis</span>
<span id="cb1-2"><a href="#cb1-2"></a>p <span class="op">=</span> dis.Normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-3"><a href="#cb1-3"></a>q <span class="op">=</span> dis.Normal(loc<span class="op">=</span><span class="fl">0.1</span>, scale<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-4"><a href="#cb1-4"></a>x <span class="op">=</span> q.sample(sample_shape<span class="op">=</span>(<span class="dv">10</span>_000_000,))</span>
<span id="cb1-5"><a href="#cb1-5"></a>truekl <span class="op">=</span> dis.kl_divergence(p, q)</span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="bu">print</span>(<span class="st">&quot;true&quot;</span>, truekl)</span>
<span id="cb1-7"><a href="#cb1-7"></a>logr <span class="op">=</span> p.log_prob(x) <span class="op">-</span> q.log_prob(x)</span>
<span id="cb1-8"><a href="#cb1-8"></a>k1 <span class="op">=</span> <span class="op">-</span>logr</span>
<span id="cb1-9"><a href="#cb1-9"></a>k2 <span class="op">=</span> logr <span class="op">**</span> <span class="dv">2</span> <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb1-10"><a href="#cb1-10"></a>k3 <span class="op">=</span> (logr.exp() <span class="op">-</span> <span class="dv">1</span>) <span class="op">-</span> logr</span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="cf">for</span> k <span class="kw">in</span> (k1, k2, k3):</span>
<span id="cb1-12"><a href="#cb1-12"></a>    <span class="bu">print</span>((k.mean() <span class="op">-</span> truekl) <span class="op">/</span> truekl, k.std() <span class="op">/</span> truekl)</span></code></pre></div>
<p><em>Thanks to Jacob Hilton and Nisan Stiennon for helpful feedback.</em></p>

</body>

<!-- Mirrored from joschu.net/blog/kl-approx.html by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 20 Mar 2021 11:24:52 GMT -->
</html>